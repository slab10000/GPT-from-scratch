{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiFGxNeuRHbC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmLj7S0SRM6k"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(256)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' #Cuda is the drivers for Nvidia GPU\n",
        "\n",
        "block_size        = 40      ## N tokens in sequence -> its not using tiktoken. Tokens are the individual letters\n",
        "batch_size        = 64\n",
        "max_iters         = 1       ## Lets try with 1 to learn\n",
        "eval_interval     = 500     # Print what it is doing\n",
        "learning_rate     = 0.0003  # Control how much we increase or decrease the weight values with the scocastic gradient descent\n",
        "eval_iters        = 300     # Evaluate every this iterations\n",
        "vocab_size        = 378      ## 65 -> 26 uppercase, 26 lowercase, digits, simbols...\n",
        "\n",
        "## every id for a given token is embedded to vector of this size\n",
        "n_embd            = 512\n",
        "n_head            = 8         ## 8 attention heads\n",
        "n_layer           = 6         ## 6 eoncoder layers\n",
        "dropout           = 0.2       # We will se this later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KO7mtTETRi4"
      },
      "outputs": [],
      "source": [
        "text = ''\n",
        "\n",
        "#input_file2 = 'HuckFinn.txt'\n",
        "input_file2 = '/content/drive/MyDrive/Colab Notebooks/dataFromRFCs.txt' # Full path\n",
        "\n",
        "with open(input_file2, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDa_KLh0UA_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d4ff2d-88d4-4925-d586-66f68e5a1367",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "3\n",
            "5\n",
            "20\n",
            "31\n",
            "33\n",
            "57\n",
            "59\n",
            "66\n",
            "86\n",
            "88\n",
            "89\n",
            "90\n",
            "124\n",
            "127\n",
            "162\n",
            "194\n",
            "270\n",
            "304\n",
            "542\n",
            "586\n",
            "640\n",
            "675\n",
            "761\n",
            "783\n",
            "787\n",
            "791\n",
            "792\n",
            "793\n",
            "804\n",
            "819\n",
            "822\n",
            "823\n",
            "826\n",
            "854\n",
            "864\n",
            "865\n",
            "867\n",
            "868\n",
            "879\n",
            "882\n",
            "889\n",
            "894\n",
            "916\n",
            "917\n",
            "918\n",
            "951\n",
            "952\n",
            "957\n",
            "959\n",
            "994\n",
            "1001\n",
            "1002\n",
            "1006\n",
            "1025\n",
            "1027\n",
            "1033\n",
            "1034\n",
            "1035\n",
            "1036\n",
            "1042\n",
            "1055\n",
            "1065\n",
            "1071\n",
            "1122\n",
            "1123\n",
            "1138\n",
            "1141\n",
            "1148\n",
            "1149\n",
            "1150\n",
            "1155\n",
            "1157\n",
            "1158\n",
            "1172\n",
            "1180\n",
            "1183\n",
            "1185\n",
            "1195\n",
            "1227\n",
            "1258\n",
            "1288\n",
            "1319\n",
            "1320\n",
            "1321\n",
            "1322\n",
            "1323\n",
            "1332\n",
            "1337\n",
            "1340\n",
            "1345\n",
            "1349\n",
            "1350\n",
            "1361\n",
            "1365\n",
            "1413\n",
            "1436\n",
            "1438\n",
            "1459\n",
            "1464\n",
            "1498\n",
            "1510\n",
            "1517\n",
            "1519\n",
            "1522\n",
            "1524\n",
            "1531\n",
            "1533\n",
            "1534\n",
            "1535\n",
            "1542\n",
            "1557\n",
            "1606\n",
            "1609\n",
            "1624\n",
            "1628\n",
            "1630\n",
            "1633\n",
            "1661\n",
            "1662\n",
            "1685\n",
            "1712\n",
            "1724\n",
            "1725\n",
            "1738\n",
            "1741\n",
            "1751\n",
            "1760\n",
            "1766\n",
            "1771\n",
            "1774\n",
            "1810\n",
            "1812\n",
            "1818\n",
            "1855\n",
            "1865\n",
            "1866\n",
            "1867\n",
            "1878\n",
            "1891\n",
            "1894\n",
            "1912\n",
            "1915\n",
            "1918\n",
            "1925\n",
            "1927\n",
            "1928\n",
            "1936\n",
            "1939\n",
            "1945\n",
            "1948\n",
            "1951\n",
            "1952\n",
            "1959\n",
            "1964\n",
            "1979\n",
            "1981\n",
            "1985\n",
            "1995\n",
            "1997\n",
            "2003\n",
            "2004\n",
            "2006\n",
            "2015\n",
            "2018\n",
            "2021\n",
            "2026\n",
            "2028\n",
            "2030\n",
            "2040\n",
            "2042\n",
            "2045\n",
            "2046\n",
            "2047\n",
            "2049\n",
            "2060\n",
            "2069\n",
            "2076\n",
            "2080\n",
            "2083\n",
            "2092\n",
            "2100\n",
            "2104\n",
            "2119\n",
            "2126\n",
            "2127\n",
            "2131\n",
            "2132\n",
            "2136\n",
            "2141\n",
            "2142\n",
            "2152\n",
            "2154\n",
            "2157\n",
            "2181\n",
            "2182\n",
            "2183\n",
            "2184\n",
            "2192\n",
            "2196\n",
            "2202\n",
            "2203\n",
            "2205\n",
            "2209\n",
            "2213\n",
            "2217\n",
            "2222\n",
            "2223\n",
            "2229\n",
            "2231\n",
            "2234\n",
            "2236\n",
            "2244\n",
            "22\n"
          ]
        }
      ],
      "source": [
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btlEoz0qUuU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1ec789a-5fa0-4bd2-f945-207f7532ef34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of data in letter or characters\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "530397136"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "print(\"length of data in letter or characters\")\n",
        "len(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "the_chars  = sorted(     list(set(text))     )\n",
        "\n",
        "vocab_size = len( the_chars )\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "x9yUmPWPerXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d53d57a7-c0d4-4a2c-e3bb-0b9cb7f15ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\".join(the_chars))"
      ],
      "metadata": {
        "id": "A6YY-B6meygv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5df9a29-2f0e-4dc7-8c8e-d54ea1d07e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u0000\u0001\u0002\b\t\n",
            "\f\u0016\u001a\u001b !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~¬£¬Æ¬∞¬µ¬π¬∫√Å√Ö√á√â√ì√ñ√ò√ú√ü√†√°√¢√£√§√•√¶√ß√®√©√™√´√¨√≠√Æ√Ø√±√≤√≥√¥√µ√∂√∏√π√∫√º√ΩƒÅƒáƒåƒçƒìƒôƒõƒüƒ´ƒ±≈Ç≈Ñ≈ë≈ô≈û≈†≈°≈´≈Ø≈Ω≈æ≈ø«ê…ë…™ í ª ºÀàÀêÃÅŒ£Œ´Œ±Œ≤ŒµŒ∏ŒºœÄœÇœÉœãœî–í–î–ò–ö–ú–û–ü–†–°–¢–£–§–¶–∞–≤–≥–¥–µ–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ü—á—â—ã—å—é—è—ë÷±÷∂÷º◊ê◊ë◊ï◊ò◊õ◊ú◊ù◊†◊©◊™◊¥‡ªê·ãê·é¢·éµ·èã·èí·èö·öÄ·∏•·π¢·π£‚Äì‚Äî‚Äù‚Ä¢‚Ç¨‚Ññ‚Ö£‚Ö≥‚Üí‚àû‚âà‚â°‚â§‚â•‚ãÖ‚åä‚åã‚åò‚éõ‚éú‚éù‚éû‚éü‚é†‚é°‚é¢‚é£‚é§‚é•‚é¶‚éß‚é®‚é©‚é™‚é´‚é¨‚é≠‚éÆ‚éØ‚é∫‚îÄ‚îÇ‚îå‚îê‚îî‚îú‚î¨‚î¥‚ïë‚ïß‚ï±‚ï≤‚ï¥‚ò∫‚ôö‚ô¶‚ü©„ÄÄ„Äí„Éé‰∏Ä‰∏ú‰∏≠‰∏∏‰∏∫‰πÖ‰∫¨‰ªô‰ª£‰øùÂÖ¨ÂÜÖÂåóÂå∫ÂçÉÂçéÂè≤Âè∏ÂëàÂíåÂõΩÂù°Â§ßÂ••Â•ΩÂ≠´Â±±ÂπøÂΩ¶ÊâÄÊäÄÊñáÊô∫ÊúâÊú¨ÊúØÊùéÊùëÊù±ÊùæÊ£âÊ∞¥ÊµÅÊπñÊ∫™Áî∞Á†îÁ©ÇËÉåËéûÈÄ∏ÈÉëÈÉΩÈôêÔ¨ÅÔªøÔºêÔøΩêÖëüÅÅüÅ≥üñ§\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Not going to use it now\n",
        "stoi = { ch:i for i, ch in enumerate(the_chars) } # Character to int\n",
        "itos = { i:ch for i, ch in enumerate(the_chars) } # Int to char"
      ],
      "metadata": {
        "id": "oN_Zq9MapEu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "WVntQwhFqiVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = tokenizer.n_vocab #50257"
      ],
      "metadata": {
        "id": "klI-28N8q3-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.n_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_bxbimvyfIe",
        "outputId": "63d61fec-e87d-40c3-fb02-e4412035c894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encode = lambda s: tokenizer.encode(s)\n",
        "encodeOld = lambda s: [ stoi[c]          for c in s   ]"
      ],
      "metadata": {
        "id": "WhkWqGJ0q98U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode(\"bahh\")) # Enconding this sheep languaje\n",
        "print(type(encode(\"bahh\"))) # Enconding this sheep languaje\n",
        "print(type(encodeOld(\"bahh\")))"
      ],
      "metadata": {
        "id": "Brwd-hH7rR3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac5079d-7d0a-461e-fd23-a4bb9142992a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[47041, 71]\n",
            "<class 'list'>\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode = lambda l: tokenizer.decode(l)\n",
        "decodeOld = lambda l: ''.join(itos[i] for i in l)"
      ],
      "metadata": {
        "id": "JH8Xsa8br8SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode([47041, 71])"
      ],
      "metadata": {
        "id": "P8WoYFl8r8Um",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a2b44f0c-1e83-4beb-9729-ac1c7d3cbe61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bahh'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(   encode(text), dtype=torch.long   ) # Enconde all the book into ints\n",
        "\n",
        "print( data )"
      ],
      "metadata": {
        "id": "bHxgOCiKr8W8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1df698d-81cd-4747-b0df-645ec53cbda5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 17, 198,  18,  ..., 220, 220, 628])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data))\n",
        "print(data.shape)"
      ],
      "metadata": {
        "id": "3j_TlnNXr8ZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "460b8dc5-c056-4f5a-f9fd-207038cc2d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "194102639\n",
            "torch.Size([194102639])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n          = int(   0.9*len(data)   )\n",
        "\n",
        "train_data = data[:n]\n",
        "val_data   = data[n:]"
      ],
      "metadata": {
        "id": "w6y1PLMRr8bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    if split == \"train\":\n",
        "        data = train_data\n",
        "    else:\n",
        "        data = val_data\n",
        "\n",
        "    ix = torch.randint(   len(data) - block_size, (batch_size,)   ) # Picks positions randomly as starting point\n",
        "\n",
        "    x  = torch.stack(    [  data[   i : i+block_size ]     for i in ix ]    ) # This 2 lines create the training and testing sets. Look the notebook *1\n",
        "    y  = torch.stack(    [  data[ i+1 : i+1+block_size ]   for i in ix ]    ) # This 2 lines create the training and testing sets. Look the notebook *1\n",
        "\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "-pT0AGDgr8eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Get batch**\n",
        "We define if our data is the training set or the validation set.\n",
        "\n",
        "We generate as many random numbers as sentences in our block."
      ],
      "metadata": {
        "id": "naWtn13q-eaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To estimate performance\n",
        "\n",
        "@torch.no_grad()    ## for efficient processing\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()   ## set to no training\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()  ## back to training\n",
        "    return out"
      ],
      "metadata": {
        "id": "QRosfW7BxOBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.key   = nn.Linear(n_embd, head_size, bias=False)  ## [512, 64]\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)  ## [512, 64]\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)  ## [512, 64]\n",
        "\n",
        "        tril_def = torch.tril( torch.ones(block_size, block_size) )  ## [40, 40]\n",
        "\n",
        "        self.register_buffer( #Marking it almost as a constant so it doesn't interfere\n",
        "                  'tril',\n",
        "                  tril_def\n",
        "               )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, E = x.shape   ## [batch_size, 40, 512]\n",
        "\n",
        "        k = self.key(   x )            ## k = (B, T, 64)\n",
        "        q = self.query( x )            ## q = (B, T, 64)\n",
        "\n",
        "        E2 = 64     ## I think this is 64 and not 512\n",
        "        ## (B, T, E) @ (B, E, T)  -> (B, T, T)\n",
        "        wei = q @ k.transpose(-2, -1) * E2 ** -0.5\n",
        "\n",
        "        wei = wei.masked_fill(\n",
        "                      self.tril[:T, :T] == 0,\n",
        "                      float('-inf')\n",
        "        )\n",
        "\n",
        "        ## (B, T, T)\n",
        "        wei = F.softmax( wei, dim= -1 )         ## (B, T, T)\n",
        "        wei = self.dropout(   wei   )\n",
        "\n",
        "        ## perform weighted aggregation of values\n",
        "\n",
        "        v   = self.value(  x  )   ## x = (B, 40, E)\n",
        "        out = wei @ v             ## (B, T, T) @ (B, T, 64) -> (B, T, 64)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "Ki8ON8y3xODt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "usYTrVl5xOGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A4psDdyhxOIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u59X8VajxOK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nyfjVJjgxOM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k-TcVhZQxOPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lBSld49GxORg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sNNvKxjmxOT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EkfU25hAxOWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7K0ZMTtVxOYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "der_veg0xOax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aDnTTMSOxOcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QpJ0bSdKr8gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p3XIkAhVr8i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "%%time\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import tiktoken\n",
        "\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(256)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' #Cuda is the drivers for Nvidia GPU\n",
        "\n",
        "block_size        = 40      ## N tokens in sequence -> its not using tiktoken. Tokens are the individual letters\n",
        "batch_size        = 64\n",
        "max_iters         = 20000\n",
        "eval_interval     = 500     # Print what it is doing\n",
        "learning_rate     = 0.0003  # Control how much we increase or decrease the weight values with the scocastic gradient descent\n",
        "eval_iters        = 300     # Evaluate every this iterations\n",
        "vocab_size        = 88      ## 65 -> 26 uppercase, 26 lowercase, digits, simbols...\n",
        "\n",
        "## every id for a given token is embedded to vector of this size\n",
        "n_embd            = 512\n",
        "n_head            = 8         ## 8 attention heads\n",
        "n_layer           = 6         ## 6 eoncoder layers\n",
        "dropout           = 0.2       # We will se this later\n",
        "\n",
        "text = ''\n",
        "\n",
        "#input_file2 = 'HuckFinn.txt'\n",
        "input_file2 = '/content/drive/MyDrive/Colab Notebooks/dataFromRFCs.txt' # Full path\n",
        "\n",
        "with open(input_file2, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"length of data in letter or characters\")\n",
        "len(text)\n",
        "\n",
        "list(set(text))\n",
        "\n",
        "the_chars  = sorted(     list(set(text))     )\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = tokenizer.n_vocab      ## 65\n",
        "\n",
        "print(  len(the_chars)  )\n",
        "\n",
        "print(  ''.join(the_chars)  )\n",
        "\n",
        "## The printed oputput\n",
        "## !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
        "\n",
        "stoi = { ch:i for i, ch in enumerate(the_chars) } # Character to int\n",
        "itos = { i:ch for i, ch in enumerate(the_chars) } # Int to char\n",
        "\n",
        "print( stoi )\n",
        "print( itos )\n",
        "\n",
        "\n",
        "\n",
        "encode = lambda s: tokenizer.encode(s)\n",
        "\n",
        "encode(\"bahh\") # Enconding this sheep languaje\n",
        "\n",
        "decode = lambda l: tokenizer.decode(l)\n",
        "decode([55, 54, 61, 61])\n",
        "\n",
        "data = torch.tensor(   encode(text), dtype=torch.long   ) # Enconde all the book into ints\n",
        "\n",
        "print( data )\n",
        "\n",
        "n          = int(   0.9*len(data)   )\n",
        "\n",
        "train_data = data[:n]\n",
        "val_data   = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    if split == \"train\":\n",
        "        data = train_data\n",
        "    else:\n",
        "        data = val_data\n",
        "\n",
        "    ix = torch.randint(   len(data) - block_size, (batch_size,)   ) # Picks positions randomly as starting point\n",
        "\n",
        "    x  = torch.stack(    [  data[   i : i+block_size ]     for i in ix ]    ) # This 2 lines create the training and testing sets. Look the notebook *1\n",
        "    y  = torch.stack(    [  data[ i+1 : i+1+block_size ]   for i in ix ]    ) # This 2 lines create the training and testing sets. Look the notebook *1\n",
        "\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "# THIS IS NOT RELEVANT TO THE HOMEWORKS, JUST CLARIFICATION!!!\n",
        "\n",
        "temp_batch_size = 4\n",
        "temp_block_size = 16\n",
        "\n",
        "## select random starting points for the 4 sentences\n",
        "ix = torch.randint(\n",
        "            len(data) - block_size,\n",
        "            (temp_batch_size,)\n",
        ")\n",
        "\n",
        "print( ix ) #Same as in the example because of the seed\n",
        "\n",
        "for index_temp in ix:\n",
        "    print(  data[index_temp]  )\n",
        "\n",
        "x  = torch.stack(\n",
        "    [ data[   i : i+  temp_block_size ]   for i in ix ]\n",
        "\n",
        ")\n",
        "\n",
        "y  = torch.stack(\n",
        "    [ data[ i+1 : i+1+ temp_block_size ]  for i in ix ]\n",
        ")\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "\n",
        "# To estimate performance\n",
        "\n",
        "@torch.no_grad()    ## for efficient processing\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()   ## set to no training\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()  ## back to training\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.key   = nn.Linear(n_embd, head_size, bias=False)  ## [512, 64]\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)  ## [512, 64]\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)  ## [512, 64]\n",
        "\n",
        "        tril_def = torch.tril( torch.ones(block_size, block_size) )  ## [40, 40]\n",
        "\n",
        "        self.register_buffer( #Marking it almost as a constant so it doesn't interfere\n",
        "                  'tril',\n",
        "                  tril_def\n",
        "               )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, E = x.shape   ## [batch_size, 40, 512]\n",
        "\n",
        "        k = self.key(   x )            ## k = (B, T, 64)\n",
        "        q = self.query( x )            ## q = (B, T, 64)\n",
        "\n",
        "        E2 = 64     ## I think this is 64 and not 512\n",
        "        ## (B, T, E) @ (B, E, T)  -> (B, T, T)\n",
        "        wei = q @ k.transpose(-2, -1) * E2 ** -0.5\n",
        "\n",
        "        wei = wei.masked_fill(\n",
        "                      self.tril[:T, :T] == 0,\n",
        "                      float('-inf')\n",
        "        )\n",
        "\n",
        "        ## (B, T, T)\n",
        "        wei = F.softmax( wei, dim= -1 )         ## (B, T, T)\n",
        "        wei = self.dropout(   wei   )\n",
        "\n",
        "        ## perform weighted aggregation of values\n",
        "\n",
        "        v   = self.value(  x  )   ## x = (B, 40, E)\n",
        "        out = wei @ v             ## (B, T, T) @ (B, T, 64) -> (B, T, 64)\n",
        "\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):         ## 512\n",
        "\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),      ## [512, 4*512] -- To give it more room to improve\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),      ## [4*512, 512]\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):    ## (8, 64)\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(  [ Head(head_size) for _ in range(num_heads) ] )\n",
        "        self.proj  = nn.Linear(n_embd, n_embd)   ## 512, 512\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat(   [ h(x) for h in self.heads ], dim = -1   )\n",
        "        out = self.proj(  out   )\n",
        "        out = self.dropout(   out   )\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head):     ## (512, 8)\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head        ## 64 --> 512/8\n",
        "\n",
        "        self.sa   = MultiHeadAttention(n_head, head_size) # Concatenate the heads\n",
        "\n",
        "        self.ffwd = FeedForward( n_embd)    ## 512\n",
        "        self.ln1  = nn.LayerNorm(n_embd)\n",
        "        self.ln2  = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(     self.ln1(x)      )\n",
        "        x = x + self.ffwd(   self.ln2(x)      )\n",
        "        return x\n",
        "\n",
        "# --------------------------- NN Architectures ---------------------------\n",
        "# We will use Object Oriented Programming\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__() # We always have this one line\n",
        "\n",
        "        # ----------------------------------- Semantic and positional Encoding -------------------------------------------------\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)   ## [65, 512]\n",
        "        self.pos_emb_table = nn.Embedding(block_size, n_embd)     ## [block, 512]\n",
        "\n",
        "        # ---------------------------------- End Semantic and positional Encoding-----------------------------------------------\n",
        "\n",
        "\n",
        "        # ********************************** Create 6 layers of Feed Fordward **********************************\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "                *[   Block(n_embd, n_head=n_head) for _ in range(n_layer)    ]\n",
        "        )\n",
        "\n",
        "        # ********************************** End Create 6 layers of FF *****************************************\n",
        "\n",
        "\n",
        "        self.ln_f    = nn.LayerNorm(  n_embd    )\n",
        "\n",
        "\n",
        "        # ----------------------------------- Last layer -------------------------------------------------\n",
        "\n",
        "        self.lm_ffw_head = nn.Linear(n_embd, vocab_size)  ## [512, 65] # FFW Layer\n",
        "\n",
        "        # ----------------------------------- End last layer ---------------------------------------------\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None): #If you dont pass anything on targets, you are in inference mode\n",
        "        B, T = idx.shape     ## (Batch, 40)\n",
        "        ## ids and targets are both (B, T) tensors of integers\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.pos_emb_table(torch.arange(T, device=device))\n",
        "\n",
        "        x = tok_emb + pos_emb    ## [B, T, E] or [64, 40, 512]\n",
        "\n",
        "        ## This is the architecture\n",
        "        x = self.blocks(  x  )   ## (B, T, E)\n",
        "        x = self.ln_f(    x  )   ## (B, T, E)   ## normalization\n",
        "        logits = self.lm_ffw_head(x)         ## [B, 40, 88] --> map to the vocabulary\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, E  = logits.shape\n",
        "            logits  = logits.view( B*T, E)\n",
        "            targets = targets.view(B*T)\n",
        "            loss    = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):    ## idx is (B, T)\n",
        "        for _ in range(max_new_tokens):\n",
        "            ## crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)    ## ## get preds\n",
        "            logits = logits[:, -1, :]    ## focus on last one (B, E)\n",
        "            probs = F.softmax(logits, dim= -1)    ## (B, E) get probs\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)     ## (B, 1) selected\n",
        "            idx = torch.cat(  (idx, idx_next), dim=1  )   ## (B, T+1) append sample to running sequence\n",
        "        return idx\n",
        "\n",
        "model   = GPTModel()\n",
        "\n",
        "m       = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(  m.parameters(), lr=learning_rate   )\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "3G2Wcq5HrWaI",
        "outputId": "ce66347c-1e6b-42a6-9307-0976fc2860d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Train the model\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    ## eval the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)   ## zero out to prevent them to accumulate\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "JTeoyjThs6IL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "78f64049-bd39-431b-c547-9ac15000074e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'estimate_loss' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3254778592.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'estimate_loss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db747002"
      },
      "source": [
        "# Assuming you have already initialized the tokenizer as 'tokenizer'\n",
        "print(tokenizer.vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_lst = encode(\"RFC 5253\")\n",
        "\n",
        "new_np = np.array(  new_lst   )\n",
        "new_np\n"
      ],
      "metadata": {
        "id": "xWF5X_b0s8tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ThlXYvlSH2Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vN68nS47H2DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mrWciguvH2F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-_g5WUk3H2IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b2yfGnjfH2Kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lB4NxNoOH2Mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JEFx4wmNH2O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s32KfcEDH2RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "khPq5KrFH2Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r0iFgsfSH2V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hAx3PSoEH2YG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NO TIKTOKEN"
      ],
      "metadata": {
        "id": "DJfaTDC8H7vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## All code in one\n",
        "\n",
        "%%time\n",
        "# Colab training with checkpoints and auto-resume (tiktoken, A100 friendly)\n",
        "\n",
        "import os, math, time, json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import tiktoken\n",
        "\n",
        "# -------------------- Repro and device --------------------\n",
        "torch.manual_seed(256)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# -------------------- Paths --------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "INPUT_FILE = '/content/drive/MyDrive/Colab Notebooks/dataFromRFCs.txt'\n",
        "CKPT_DIR   = '/content/drive/MyDrive/Colab Notebooks/checkpoints_rfc_tiktoken'\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------- Hyperparams --------------------\n",
        "# A100 friendly defaults\n",
        "block_size     = 512\n",
        "batch_size     = 64\n",
        "grad_accum     = 2             # effective batch is 64 * 512 * 2 tokens per step\n",
        "max_iters      = 135_000\n",
        "eval_interval  = 500\n",
        "eval_iters     = 200\n",
        "learning_rate  = 3e-4\n",
        "dropout        = 0.2\n",
        "\n",
        "n_embd         = 512\n",
        "n_head         = 8\n",
        "n_layer        = 6\n",
        "\n",
        "# -------------------- Data --------------------\n",
        "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "# tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "# vocab_size = tokenizer.n_vocab  # 50257\n",
        "\n",
        "# encode = lambda s: tokenizer.encode(s)\n",
        "#¬†decode = lambda ids: tokenizer.decode(ids)\n",
        "\n",
        "the_chars  = sorted(list(set(raw_text)))\n",
        "vocab_size = len(the_chars)\n",
        "\n",
        "stoi = { ch:i for i, ch in enumerate(the_chars) } # Character to int\n",
        "itos = { i:ch for i, ch in enumerate(the_chars) } # Int to char\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join(itos[i] for i in l)\n",
        "\n",
        "\n",
        "\n",
        "ids = encode(raw_text)\n",
        "data = torch.tensor(ids, dtype=torch.long)\n",
        "\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data   = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    src = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(src) - block_size, (batch_size,))\n",
        "    x  = torch.stack([src[i : i + block_size] for i in ix])\n",
        "    y  = torch.stack([src[i + 1 : i + 1 + block_size] for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters, device=device)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss\n",
        "        out[split] = losses.mean().item()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# -------------------- Model --------------------\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        tril = torch.tril(torch.ones(block_size, block_size))\n",
        "        self.register_buffer('tril', tril)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, E = x.shape\n",
        "        k = self.key(x)      # (B, T, H)\n",
        "        q = self.query(x)    # (B, T, H)\n",
        "        H = q.size(-1)\n",
        "        wei = q @ k.transpose(-2, -1) * (H ** -0.5)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)    # (B, T, H)\n",
        "        out = wei @ v        # (B, T, H)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj  = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa   = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1  = nn.LayerNorm(n_embd)\n",
        "        self.ln2  = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos_emb_table         = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f   = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, E)\n",
        "        pos_emb = self.pos_emb_table(torch.arange(T, device=idx.device))  # (T, E)\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)  # (B, T, V)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, V = logits.shape\n",
        "            loss = F.cross_entropy(logits.view(B*T, V), targets.view(B*T))\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_new_tokens):\n",
        "                idx_cond = idx[:, -block_size:]\n",
        "                logits, _ = self(idx_cond)\n",
        "                logits = logits[:, -1, :]\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "                idx = torch.cat((idx, idx_next), dim=1)\n",
        "        self.train()\n",
        "        return idx\n",
        "\n",
        "# -------------------- Init --------------------\n",
        "model = GPTModel().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Mixed precision for speed and memory\n",
        "use_amp = torch.cuda.is_available()\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "# Optional cosine schedule with warmup\n",
        "warmup_steps = 2_000\n",
        "total_steps  = max_iters\n",
        "def get_lr(step):\n",
        "    if step < warmup_steps:\n",
        "        return learning_rate * (step + 1) / warmup_steps\n",
        "    # cosine decay to 10 percent of base\n",
        "    progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
        "    return 0.1 * learning_rate + 0.9 * learning_rate * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "# -------------------- Checkpoint helpers --------------------\n",
        "def save_checkpoint(tag, step, best_val):\n",
        "    tmp = os.path.join(CKPT_DIR, f\"ckpt_{tag}.pt.tmp\")\n",
        "    path = os.path.join(CKPT_DIR, f\"ckpt_{tag}.pt\")\n",
        "    payload = {\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"scaler\": scaler.state_dict() if use_amp else None,\n",
        "        \"step\": step,\n",
        "        \"best_val\": best_val,\n",
        "        \"cfg\": {\n",
        "            \"block_size\": block_size,\n",
        "            \"n_embd\": n_embd, \"n_head\": n_head, \"n_layer\": n_layer,\n",
        "            \"dropout\": dropout, \"vocab_size\": vocab_size,\n",
        "            \"tokenizer_name\": \"gpt2\",\n",
        "        },\n",
        "    }\n",
        "    torch.save(payload, tmp)\n",
        "    os.replace(tmp, path)\n",
        "    return path\n",
        "\n",
        "def load_checkpoint_if_exists():\n",
        "    latest = os.path.join(CKPT_DIR, \"ckpt_latest.pt\")\n",
        "    if os.path.exists(latest):\n",
        "        ckpt = torch.load(latest, map_location=device)\n",
        "        model.load_state_dict(ckpt[\"model\"], strict=True)\n",
        "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "        if use_amp and ckpt.get(\"scaler\") is not None:\n",
        "            scaler.load_state_dict(ckpt[\"scaler\"])\n",
        "        step = ckpt.get(\"step\", 0)\n",
        "        best_val = ckpt.get(\"best_val\", float(\"inf\"))\n",
        "        print(f\"Resumed from step {step}, best_val {best_val:.4f}\")\n",
        "        return step, best_val\n",
        "    print(\"No existing checkpoint, starting fresh\")\n",
        "    return 0, float(\"inf\")\n",
        "\n",
        "# -------------------- Resume if possible --------------------\n",
        "start_iter, best_val = load_checkpoint_if_exists()\n",
        "\n",
        "# -------------------- Quick sanity sample before training --------------------\n",
        "with torch.no_grad():\n",
        "    ctx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "    sample_ids = model.generate(ctx, max_new_tokens=64)[0].tolist()\n",
        "print(decode(sample_ids))\n",
        "\n",
        "# -------------------- Training loop --------------------\n",
        "save_every = 5_000\n",
        "t0 = time.time()\n",
        "\n",
        "model.train()\n",
        "for iter in range(start_iter, max_iters):\n",
        "\n",
        "    # LR schedule\n",
        "    for g in optimizer.param_groups:\n",
        "        g['lr'] = get_lr(iter)\n",
        "\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss(model)\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, lr {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "        save_checkpoint(\"latest\", iter, best_val)\n",
        "        if losses['val'] < best_val:\n",
        "            best_val = losses['val']\n",
        "            save_checkpoint(\"best\", iter, best_val)\n",
        "            print(\"Saved new best checkpoint\")\n",
        "\n",
        "    if iter % save_every == 0 and iter > start_iter:\n",
        "        save_checkpoint(f\"step{iter}\", iter, best_val)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # Gradient accumulation\n",
        "    total_loss = 0.0\n",
        "    for _ in range(grad_accum):\n",
        "        xb, yb = get_batch('train')\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            logits, loss = model(xb, yb)\n",
        "            loss = loss / grad_accum\n",
        "        scaler.scale(loss).backward()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "# Final save\n",
        "save_checkpoint(\"latest\", max_iters, best_val)\n",
        "print(\"Training done, total time min:\", (time.time() - t0) / 60.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "bRE_iXFaIBzt",
        "outputId": "093dbac6-ffbe-42c1-e9dd-7cf5a1a8e222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<timed exec>:189: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumed from step 135000, best_val 0.7006\n",
            "\u0000\n",
            "   3.  Content Format for Word characters......................\n",
            "step 135000: train loss 0.5889, val loss 0.7072, lr 0.000030\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<timed exec>:275: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, torch, tiktoken\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Recreate the model class definitions exactly as in training\n",
        "# If this runs in the same notebook session, you already have them\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "CKPT_DIR = '/content/drive/MyDrive/Colab Notebooks/checkpoints_rfc_tiktoken' # Corrected path\n",
        "best_path = os.path.join(CKPT_DIR, 'ckpt_best.pt')\n",
        "\n",
        "# Load config and weights\n",
        "ckpt = torch.load(best_path, map_location=device)\n",
        "cfg = ckpt['cfg']\n",
        "block_size = cfg['block_size']\n",
        "n_embd = cfg['n_embd']\n",
        "n_head = cfg['n_head']\n",
        "n_layer = cfg['n_layer']\n",
        "dropout = cfg['dropout']\n",
        "# vocab_size is loaded from checkpoint cfg\n",
        "print(\"Best validation loss:\", ckpt[\"best_val\"])\n",
        "print(\"Step:\", ckpt[\"step\"])\n",
        "print()\n",
        "\n",
        "# ---- No Tokenizer ----\n",
        "# tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "# vocab_size = tokenizer.n_vocab  # 50257\n",
        "\n",
        "# encode = lambda s: tokenizer.encode(s)\n",
        "#¬†decode = lambda ids: tokenizer.decode(ids)\n",
        "\n",
        "# Load raw_text to build vocabulary for inference\n",
        "INPUT_FILE = '/content/drive/MyDrive/Colab Notebooks/dataFromRFCs.txt'\n",
        "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "the_chars  = sorted(list(set(raw_text)))\n",
        "vocab_size = len(the_chars) # Update vocab_size based on raw_text\n",
        "\n",
        "stoi = { ch:i for i, ch in enumerate(the_chars) } # Character to int\n",
        "itos = { i:ch for i, ch in enumerate(the_chars) } # Int to char\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join(itos[i] for i in l)\n",
        "\n",
        "\n",
        "# Rebuild model and load weights\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        tril = torch.tril(torch.ones(block_size, block_size))\n",
        "        self.register_buffer('tril', tril)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        B, T, E = x.shape\n",
        "        k = self.key(x); q = self.query(x); H = q.size(-1)\n",
        "        wei = q @ k.transpose(-2, -1) * (H ** -0.5)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        return wei @ v\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj  = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa   = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1  = nn.LayerNorm(n_embd)\n",
        "        self.ln2  = nn.LayerNorm(n_embd)\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos_emb_table         = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f   = nn.LayerNorm(n_embd)\n",
        "        self.lm_head= nn.Linear(n_embd, vocab_size)\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.pos_emb_table(torch.arange(T, device=idx.device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, V = logits.shape\n",
        "            loss = F.cross_entropy(logits.view(B*T, V), targets.view(B*T))\n",
        "        return logits, loss\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_new_tokens):\n",
        "                idx_cond = idx[:, -block_size:]\n",
        "                logits, _ = self(idx_cond)\n",
        "                logits = logits[:, -1, :]\n",
        "                probs = torch.softmax(logits, dim=-1)\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "                idx = torch.cat((idx, idx_next), dim=1)\n",
        "        self.train(); return idx\n",
        "\n",
        "model = GPTModel().to(device)\n",
        "model.load_state_dict(ckpt['model'], strict=True)\n",
        "model.eval()\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    new_lst = encode(\"What is IP?\")\n",
        "    new_np = np.array(  new_lst   )\n",
        "    new_context = torch.tensor(new_np, dtype=torch.long, device=device ).unsqueeze(0) # Added unsqueeze(0)\n",
        "    out_ids = model.generate(new_context, max_new_tokens=400)[0].tolist()\n",
        "\n",
        "print(decode(out_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL-WB01dJijh",
        "outputId": "bd968a02-4905-4a3c-82a7-df8194d98d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best validation loss: 0.7005828619003296\n",
            "Step: 118000\n",
            "\n",
            "What is IP?\n",
            "           If the packet is to be PIDF, the reporting point MUST\n",
            "           resend a CONDSTORE mailbox.\n",
            "           If the name is OPEN, the reported action MUST be removed.\n",
            "\n",
            "      *    If the CONDSTORE mailbox is NOT usable and the entry is\n",
            "           dsult (the carrier issues the CONDSTORE mailbox to be locked\n",
            "           with any opened file name and changes its own mailbox), then\n",
            "           whi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate\n",
        "with torch.no_grad():\n",
        "    new_lst = encode(\"IP address is\")\n",
        "    new_np = np.array(  new_lst   )\n",
        "    new_context = torch.tensor(new_np, dtype=torch.long, device=device ).unsqueeze(0) # Added unsqueeze(0)\n",
        "    out_ids = model.generate(new_context, max_new_tokens=1000)[0].tolist()\n",
        "\n",
        "print(decode(out_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ikppgO_-n1y",
        "outputId": "9cf112f9-96a4-4855-d2e6-efc0ac46c834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IP address is\n",
            "   unable to pay a second line for the retry at which only one endpoint\n",
            "   has a line wreak buffer.\n",
            "\n",
            "   Since keys are derived from the target, it is possible to determine the\n",
            "   client's specified sequence number and the server type that may be\n",
            "   unique among the specified sets of sequence numbers it receives.\n",
            "   For this reason, the client may know that it wishes to probe the\n",
            "   issue.  If the string is associated with the specified sequence number\n",
            "   into the endpoint's sequence, the client SHOULD add a literal literal\n",
            "   to the user's sequence number for the specified address of the\n",
            "   client.\n",
            "\n",
            "   The string in the endpoint's specified Sequence Number is used as a\n",
            "   connection ID, which MUST be present in the endpoint's sequence\n",
            "   number into the specified type.\n",
            "\n",
            "11.4.  CSNP Endpoints\n",
            "\n",
            "   Although all Sequences Numbers are subject to the issues discussed in\n",
            "   [RFC4987], it is possible for the client to verify the ciphertext\n",
            "   value with the CSNP Endpoint ID.  Individual endpo\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}